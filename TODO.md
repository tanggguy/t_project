TODO COMPLET - Projet Trading Python : comment√© = fait
<!-- üìã Phase 1 : Setup Initial (Semaine 1)
    1.1 Environnement de d√©veloppement

    Cr√©er un nouveau repository Git
    Initialiser un environnement virtuel Python (venv ou conda)
    Cr√©er le fichier requirements.txt avec les d√©pendances de base :

    yfinance
    backtrader
    pandas-ta
    optuna
    pandas
    numpy
    matplotlib
    plotly
    pyyaml
    python-dotenv
    jupyter
    pytest

    Installer toutes les d√©pendances
    Cr√©er la structure de dossiers du projet
    Configurer .gitignore pour Python
    Cr√©er README.md avec description du projet
    Setup du logging de base (utils/logger.py)

    1.2 Configuration

    Cr√©er config/settings.yaml avec param√®tres globaux :

    P√©riodes par d√©faut
    Commissions broker
    Capital initial
    Timezone

    Cr√©er .env pour API keys (si n√©cessaire plus tard)
    Cr√©er utils/config_loader.py pour charger les configurations

    üìä Phase 2 : Gestion des Donn√©es (Semaine 1-2)
    2.1 Data Manager

    Cr√©er utils/data_manager.py avec classe DataManager :

    M√©thode download_data() pour yfinance
    M√©thode save_to_cache() pour sauvegarder en CSV
    M√©thode load_from_cache() pour charger depuis cache
    Gestion des erreurs de t√©l√©chargement
    Validation des donn√©es (trous, valeurs aberrantes)

    2.2 Scripts de donn√©es

    Cr√©er scripts/download_data.py :

    Arguments CLI (ticker, p√©riode, intervalle)
    Mode batch pour t√©l√©charger multiple tickers
    Barre de progression pour t√©l√©chargements

    Cr√©er liste de tickers par march√© :

    config/markets/sp500.yaml
    config/markets/cac40.yaml

    2.3 Data preprocessing

    Cr√©er utils/data_processor.py :

    Calcul des returns
    D√©tection et gestion des outliers
    Resampling (aggr√©gation temporelle)

    2.4 Validation

    Notebook 01_data_exploration.ipynb :

    Visualisation des donn√©es t√©l√©charg√©es
    Statistiques descriptives
    V√©rification de la qualit√© des donn√©es
    Test de t√©l√©chargement sur 5-10 tickers

    üéØ Phase 3 : Premi√®re Strat√©gie Simple (Semaine 2-3)
    3.1 Base Strategy

    Cr√©er strategies/base_strategy.py :

    Classe abstraite h√©ritant de bt.Strategy
    M√©thodes template : __init__, next, notify_order
    Logging int√©gr√©
    Gestion basique des ordres

    3.2 Strat√©gie Moving Average Crossover

    Cr√©er strategies/implementations/ma_crossover.py :

    Param√®tres : fast_period, slow_period
    Logique : achat sur golden cross, vente sur death cross
    Position sizing simple (100% du capital)
    Pas de stop-loss pour commencer

    3.3 Premier Backtest

    Cr√©er backtesting/engine.py :

    Classe BacktestEngine
    Configuration Cerebro (capital, commission)
    Ajout des analyseurs basiques (returns, sharpe)
    M√©thode run() qui retourne les r√©sultats

    3.4 Script de test

    Cr√©er scripts/run_backtest.py :
    Scanne automatiquement strategies/implementations/
    D√©tecte toutes les classes h√©ritant de BaseStrategy
    Affiche les param√®tres par d√©faut de chaque strat√©gie
    3Ô∏è‚É£ Param√®tres par D√©faut Automatiques
    python scripts/run_backtest.py --config config/backtest_config.yaml

    Charger donn√©es d'un ticker (ex: AAPL)
    Lancer backtest sur 2 ans
    Afficher r√©sultats basiques (P&L, nombre trades)

    V√©rifier que tout fonctionne bout en bout

    üìà Phase 4 : Stategie (Semaine 3-4)

    Strat√©gies avec indicateurs

    Cr√©er strategies/implementations/rsi_oversold.py :

    Achat sur RSI < 30, vente sur RSI > 70

    Cr√©er strategies/implementations/macd_momentum.py :

    Trading sur croisements MACD

 Notebook 02_strategy_development.ipynb :

 Tests visuels des indicateurs
 Backtests comparatifs

    üí∞ Phase 5 : Risk Management (Semaine 4-5)
    5.1 Stop Loss et Take Profit

    Cr√©er risk_management/stop_loss.py :

    Fixed stop loss (%)
    Trailing stop loss
    ATR-based stop loss
    Support/Resistance stops

    Cr√©er risk_management/take_profit.py

    5.2 Position Sizing

    Cr√©er risk_management/position_sizing.py :

    Fixed fractional (risquer X% par trade)
    
    Volatility-based sizing
 

    5.3 Int√©gration

    Modifier base_strategy.py pour int√©grer risk management
    Ajouter param√®tres de risque dans configs

    üîß Phase 6 : Optimisation Basique (Semaine 5-6)
    6.1 Setup Optuna

    Cr√©er optimization/optuna_optimizer.py :

    Classe OptunaOptimizer
    D√©finition de l'espace de recherche
    Fonction objectif (maximize Sharpe ratio)
    Sauvegarde des √©tudes

    6.2 Parameter Spaces

    Cr√©er optimization/parameter_spaces.py :

    Espaces pour MA Crossover
    Espaces pour RSI strategy
    Contraintes et d√©pendances

    6.3 Premi√®re optimisation

    Script scripts/run_optimization.py :

    Optimiser MA Crossover sur donn√©es historiques
    100 trials minimum
    Sauvegarder meilleurs param√®tres

    Visualisation des r√©sultats Optuna

    üìä Phase 7 : M√©triques et Analyse (Semaine 6-7)
    7.1 Analyzers avanc√©s

    Cr√©er backtesting/analyzers/performance.py :

    Sharpe, Sortino, Calmar ratios
    Win rate, Profit factor
    Average trade, Best/Worst trade

    7.2 Drawdown analysis

    Cr√©er backtesting/analyzers/drawdown.py :

    Maximum drawdown
    Dur√©e des drawdowns
    Recovery time
    Underwater curve

    7.3 Reporting

    Cr√©er reports/report_generator.py :

    Template HTML pour rapports
    Graphiques performance
    Tableau des trades
    Export PDF -->

<!-- üé® Phase 8 : Visualisation (Semaine 7-8)
8.1 Charts de base

 Cr√©er visualization/charts.py :

 Candlestick 
 Points d'entr√©e/sortie

 est ce que 8.1 vaut le coup, que apporter en plus du plot natif de backtrader ? -->

<!-- 8.2 Dashboard

 Cr√©er visualization/dashboard.py :

 Dashboard Plotly/Dash
 Comparaison multi-strat√©gies
 M√©triques temps r√©el
 S√©lection p√©riode analyse

8.3 Notebook d'analyse

 03_backtest_analysis.ipynb :

 Analyse d√©taill√©e des trades
 Patterns gagnants/perdants
 Analyse par p√©riode -->

<!-- üöÄ Phase 9 : Optimisation Avanc√©e (Semaine 8-9) -->
<!-- 9.1 Overfitting prevention

 Cr√©er optimization/overfitting_check.py :

 Walk-forward analysis
 Out-of-sample testing
 Monte Carlo simulation
 Stability tests -->

<!-- esquisser scripts/run_overfitting.py pr√™t √† l‚Äôemploi, -->

<!-- 9.2 Multi-objective

 Modifier optimization/objectives.py :

 Optimisation multi-objectifs
 Trade-off return/risque
 Contraintes custom -->
<!-- Amelioration module overffiting :
    1. Conception des m√©triques d‚Äôoverfitting / robustesse

    D√©finir les indicateurs WFA/OOS cibles
    degradation_ratio = test_sharpe_mean / train_sharpe_mean
    test_vs_train_gap = test_sharpe_mean - train_sharpe_mean
    Fr√©quence de folds ‚Äúmauvais‚Äù :
    - [ ] frac_test_sharpe_lt_0 = proportion de folds avec test_sharpe < 0
    - [ ] frac_test_sharpe_lt_alpha_train = proportion de folds avec test_sharpe < Œ± * train_sharpe (choisir Œ±, ex. 0.5)
    D√©finir les zones qualitatives (‚Äúbadge‚Äù de robustesse)
    Choisir des seuils pour degradation_ratio et frac_test_sharpe_lt_alpha_train, ex :
    - [ ] Robuste : degradation_ratio >= 0.8 et frac_test_sharpe_lt_alpha_train <= 0.2
    - [ ] Borderline : entre les 2 zones
    - [ ] Sur‚Äëajust√© : degradation_ratio <= 0.5 ou frac_test_sharpe_lt_alpha_train >= 0.5
    D√©finir les indicateurs Monte Carlo cibles
    p_sharpe_lt_0 = proportion des simulations avec sharpe_ratio < 0
    p_cagr_lt_0 = proportion des simulations avec cagr < 0
    
    <!-- D√©cider o√π stocker les scores de synth√®se
    Ajouter un petit bloc robustness_summary dans les dictionnaires summary WFA/OOS/Monte Carlo/stabilit√©
    <!-- Pr√©voir d‚Äôutiliser ces champs dans overfitting_report.render_overfitting_index pour afficher les badges --> -->
<!-- 2. Impl√©mentation des nouvelles m√©triques c√¥t√© OverfittingChecker

    Enrichir le r√©sum√© WFA (optimization/overfitting_check.py)
    Dans walk_forward_analysis, apr√®s calcul des listes train_sharpes / test_sharpes et fold_results :
    Calculer degradation_ratio, test_vs_train_gap
    Calculer frac_test_sharpe_lt_0 et frac_test_sharpe_lt_alpha_train
    Ajouter ces valeurs dans summary (ex. cl√©s "degradation_ratio", "frac_test_sharpe_lt_0", etc.)
    D√©terminer un champ de statut global WFA : "robustness_label": "robust" | "borderline" | "overfitted"
    Enrichir les tests OOS
    Dans out_of_sample_test, √† partir des sharpe_values :
    Calculer frac_oos_sharpe_lt_0
    Calculer √©ventuellement oos_degradation_ratio si tu peux rapprocher oos_sharpe_mean d‚Äôun train_sharpe_mean (option : utiliser la moyenne des Sharpe train sur la p√©riode globale)
    Ajouter ces statistiques dans summary (et un √©ventuel oos_robustness_label)
    Enrichir le r√©sum√© Monte Carlo
    Dans _summarize_simulations, √† partir du DataFrame df :
    Ajouter p_sharpe_lt_0 = (df["sharpe_ratio"] < 0).mean()
    Ajouter p_cagr_lt_0 = (df["cagr"] < 0).mean()
    Si d√©cision sur un seuil de drawdown, ajouter p_max_dd_gt_threshold
    (Optionnel) Calculer un monte_carlo_robustness_label bas√© sur ces probabilit√©s
    Enrichir la stabilit√©
    Dans stability_tests, √† partir de summary et neighbors :
    V√©rifier que robust_fraction est bien l‚Äôindicateur principal --> -->
<!-- Ajouter un label stability_robustness_label bas√© sur robust_fraction (ex. robuste si ‚â• 0.7, sur‚Äëajust√© si ‚â§ 0.4) -->
<!-- 3. Propagation des nouvelles m√©triques dans les exports (CSV / HTML)

    Mettre √† jour _export_wfa_results
    Ajouter les nouvelles colonnes dans summary_df (degradation_ratio, frac_test_sharpe_lt_0, etc.)
    Option : ajouter les indicateurs de ‚Äúmauvais fold‚Äù par ligne si utile (ex. bool√©en is_bad_fold)
    Mettre √† jour _export_oos_results
    Ajouter les colonnes OOS dans summary_df (frac_oos_sharpe_lt_0, oos_robustness_label, ‚Ä¶)
    S‚Äôassurer que les CSV gardent un format simple (une seule ligne de r√©sum√©)
    Mettre √† jour _export_monte_carlo
    Ajouter p_sharpe_lt_0, p_cagr_lt_0, etc. aux colonnes de summary_df
    V√©rifier que les CSV restent lisibles et exploitables (nom de colonnes explicite)
    Mettre √† jour _export_stability
    Ajouter stability_robustness_label dans summary_df
    (Option) Ajouter un summary.json global
    Cr√©er un helper qui agr√®ge les summary de WFA/OOS/Monte Carlo/Stability
    Sauvegarder ce JSON dans self.output_root / "summary.json" (utile pour des dashboards externes) --> -->
<!-- 4. Enrichissement de l‚Äôindex Overfitting HTML

    Adapter _register_report_section (optimization/overfitting_check.py)
    √âtendre l‚Äôentr√©e entry pour inclure un champ optionnel status (ex. "robust", "borderline", "overfitted")
    Passer ce status au moment de l‚Äôappel pour chaque type de rapport (WFA/OOS/Monte Carlo/Stability)
    Adapter render_overfitting_index (reports/overfitting_report.py)
    Modifier la signature pour accepter un status par section (conserver la r√©tro‚Äëcompatibilit√©)
    Dans la g√©n√©ration HTML des cartes :
    Afficher un badge color√© en fonction de status (par ex. petit <span> avec classes CSS)
    D√©finir dans_BASE_STYLE des styles simples pour les badges :
    - badge-robust (vert doux)
    - badge-borderline (orange)
    - badge-overfitted (rouge)
    Ajouter √©ventuellement un r√©sum√© global dans la section ‚ÄúMeta‚Äù (ex. ‚ÄúGlobal: Borderline (WFA robuste, MC fragile)‚Äù si tu veux fusionner les labels) -->
<!-- 5. Nouveaux graphiques WFA/OOS/Monte Carlo/Stability

    Histogramme des Sharpe OOS
    Cr√©er une fonction render_oos_report dans reports/overfitting_report.py (analogue √† render_wfa_report) :
    Param√®tres : summary_df, windows_df, output_path
    Section ‚ÄúR√©sum√©‚Äù : tableau des stats OOS (d√©j√† existant)
    Section ‚ÄúHistogramme Sharpe OOS‚Äù :
    - [ ] Si go disponible : go.Histogram(x=windows_df["sharpe_ratio"])
    - [ ] Sinon : pas de plot (simple fallback texte)
    Section ‚ÄúD√©tails des fen√™tres‚Äù : table HTML windows_df
    Modifier_export_oos_results pour utiliser render_oos_report √† la place de _build_html_report
    Distribution des max drawdowns Monte Carlo
    Cr√©er une fonction render_monte_carlo_report :
    Param√®tres : summary_df, simulations_df, output_path
    Section ‚ÄúR√©sum√©‚Äù (table)
    Section ‚ÄúHistogramme Sharpe‚Äù et/ou ‚ÄúHistogramme Max Drawdown‚Äù :
    - [ ] Utiliser Plotly si dispo, sinon fallback texte
    Section ‚ÄúD√©tails des simulations‚Äù : table HTML
    Modifier _export_monte_carlo pour appeler render_monte_carlo_report
    Heatmap relative_sharpe vs param√®tre (stabilit√©)
    Cr√©er une fonction render_stability_report :
    Param√®tres : summary_df, neighbors_df, output_path
    Section ‚ÄúR√©sum√©‚Äù (table)
    Pour la heatmap :
    - [ ] Utiliser neighbors_df avec colonnes param_name, param_value, relative_sharpe
    - [ ] Construire une matrice (par exemple, une heatmap par param√®tre : x = param_value, y = param_name)
    - [ ] Avec Plotly : go.Heatmap ou une s√©rie de go.Scatter si c‚Äôest plus simple
    - [ ] Prevoir fallback sans plot si go absent
    Section ‚ÄúD√©tails des voisins‚Äù : table HTML
    Modifier _export_stability pour appeler render_stability_report au lieu de _build_html_report
    Conserver_build_html_report comme fallback g√©n√©rique
    Garder _build_html_report pour des usages simples (ou comme secours si Plotly √©choue) -->
<!-- 6. tests -->
<!-- 7. Documentation & ergonomie

    Mettre √† jour doc/optimization.md (section ‚ÄúPr√©vention de l‚Äôoverfitting‚Äù)
    D√©crire les nouveaux indicateurs de robustesse (formules, interpr√©tation)
    Ajouter un exemple de lecture de l‚Äôindex HTML avec les badges
    Mettre √† jour README.md
    Mentionner explicitement que le module d‚Äôoverfitting fournit :
    - [ ] Ratios de d√©gradation, probabilit√©s de sur‚Äëajustement, p‚Äëvalues Monte Carlo
    - [ ] Rapports HTML enrichis avec graphiques
    (Option) Ajouter un petit paragraphe explicatif dans config/overfitting_*.yaml
    Rappeler la signification des nouveaux indicateurs / seuils si certains sont param√©trables (ex. seuil drawdown, Œ±) -->

<!-- Dashboard web  de lancement des optimisation :
-lancer des optimisations avec choix des strat√©gies, tickers, p√©riodes et grille hyperparam√®tres
-estimation du temps restant
-visualisation ou lien vers des rapports html du backtest du meilleur essai
-visualisation ou lien des rapports d'overfitting

1. Architecture g√©n√©rale & emplacement

 D√©cider de l‚Äôemplacement principal du dashboard Streamlit:visualization/dashboard.py (UI uniquement, sans logique m√©tier Optuna/Backtest).
 Introduire un petit module de ‚Äúservice‚Äù r√©utilisable pour lancer/monitorer les optimisations, par ex. optimization/dashboard_runner.py, appel√© √† la fois par Streamlit et √©ventuellement par d‚Äôautres outils.
 V√©rifier que tout nouveau code respecte le style PEP8, les annotations de type (typing), et utilise logging plut√¥t que print().
2. API Python propre pour lancer une optimisation (sans casser la CLI)

 Extraire dans scripts/run_optimization.py une fonction de haut niveau, par ex. run_optimization_from_yaml(config_path: str, *, n_trials: int | None = None, timeout: int | None = None, n_jobs: int | None = None, show_progress_bar: bool | None = None) -> optuna.Study, qui :
 Utilise load_config() + build_optimizer() (d√©j√† existants),
 Appelle optimizer.optimize(...) avec les bons param√®tres,
 Ne fait aucun print() (la fonction retourne l‚Äôoptuna.Study).
 Adapter main() dans scripts/run_optimization.py pour :
 Continuer √† parser les arguments CLI exactement comme aujourd‚Äôhui,
 Appeler run_optimization_from_yaml(...),
 G√©rer l‚Äôaffichage CLI (prints) uniquement dans main() pour ne pas polluer l‚ÄôAPI Python.
 V√©rifier que l‚Äôex√©cution via CLI (python scripts/run_optimization.py --config ...) donne exactement les m√™mes sorties qu‚Äôavant (non-r√©gression fonctionnelle).
3. Service de gestion d‚Äôun ‚Äújob d‚Äôoptimisation‚Äù

Dans un nouveau module (ex. optimization/dashboard_runner.py) :

 D√©finir une dataclass, ex. OptimizationJobConfig, avec type hints, pour encapsuler :
 config_path: Path,
 n_trials, timeout, n_jobs,
 study_name, storage_url (d√©riv√©s de la config YAML via OptunaOptimizer / study_config).
 D√©finir une dataclass OptimizationJobStatus (ou similaire) avec :
 status: Literal["idle", "running", "done", "failed"],
 n_trials_planned: int | None,
 n_trials_completed: int,
 avg_trial_duration: float | None,
 eta_seconds: float | None,
 best_value: float | None,
 best_params: dict[str, Any] | None,
 last_update: datetime | None,
 √©ventuellement error_message: str | None.
 Impl√©menter une fonction start_optimization_job(job_cfg: OptimizationJobConfig) -> None qui :
 D√©marre l‚Äôoptimisation dans un process s√©par√© (ex. multiprocessing.Process ou subprocess.Popen qui appelle la CLI), pour ne pas bloquer le thread Streamlit,
 Cr√©e un fichier de ‚Äúlock‚Äù ou un √©tat persistant simple (ex. tmp-output/current_optimization.json) indiquant qu‚Äôun job est en cours.
 Impl√©menter une fonction de lecture de statut :
get_optimization_status(job_cfg: OptimizationJobConfig) -> OptimizationJobStatus qui :
 Charge l‚Äô√©tude avec optuna.load_study(study_name=..., storage=...),
 Calcule n_trials_completed = len([t for t in study.trials if t.state.is_finished()]),
 D√©termine n_trials_planned √† partir de la config YAML (study_config["n_trials"] ou param override),
 Calcule la dur√©e moyenne par trial √† partir de datetime_start / datetime_complete,
 En d√©duit une ETA simple (n_planned - n_completed) * avg_duration,
 R√©cup√®re best_value / best_params si disponibles,
 G√®re les cas edge (aucun trial termin√©, √©tude absente, job en erreur) proprement, avec logging.
 (Optionnel) Ajouter un petit cache en m√©moire ou fichier JSON pour √©viter de recharger l‚Äô√©tude trop fr√©quemment si cela s‚Äôav√®re co√ªteux.
4. S√©lection des strat√©gies, tickers, p√©riodes, hyperparam√®tres dans le dashboard

Dans visualization/dashboard.py (code Streamlit) :

 Cr√©er une fonction load_available_optimization_configs() -> dict[str, Path] qui :
 Liste les fichiers config/optimization_*.yaml,
 Retourne un mapping ‚Äúnom lisible‚Äù ‚Üí chemin du YAML.
 Ajouter un s√©lecteur Streamlit (ex. st.selectbox) pour choisir un fichier d‚Äôoptimisation YAML.
 Charger la config s√©lectionn√©e et afficher :
 Nom de la strat√©gie, module, class,
 Tickers (mono/multi),
 P√©riode (start_date / end_date / interval),
 Param√®tres d‚ÄôOptuna (n_trials, timeout, n_jobs).
 Permettre d‚Äôoverrider certains champs simples dans l‚ÄôUI, dans l‚Äôesprit KISS :
 n_trials, timeout, n_jobs,
 √©ventuellement tickers, start_date, end_date (en restant prudents pour ne pas sur-complexifier).
 Construire un OptimizationJobConfig √† partir de la config YAML + overrides UI, et l‚Äôutiliser pour start_optimization_job(...).
5. Estimation du temps restant & affichage en temps r√©el

Toujours dans visualization/dashboard.py :

 Mettre en place une section ‚ÄúSuivi de l‚Äôoptimisation en cours‚Äù :
 Utiliser st_autorefresh() ou un timer pour rafra√Æchir le statut toutes les X secondes (ex. 5‚Äì10s),
 Appeler get_optimization_status(job_cfg) √† chaque rafra√Æchissement.
 Afficher :
 Une barre de progression bas√©e sur n_trials_completed / n_trials_planned,
 L‚ÄôETA human-readable (minutes / heures restantes) √† partir de eta_seconds,
 La meilleure valeur trouv√©e (best_value) et quelques params cl√©s (best_params).
 G√©rer les √©tats :
 idle ‚Üí message ‚ÄúAucune optimisation en cours‚Äù,
 running ‚Üí progression + ETA,
 done ‚Üí message de succ√®s + liens vers rapports,
 failed ‚Üí message d‚Äôerreur lisible (error_message), avec logs.
6. Backtest HTML du meilleur essai

 R√©utiliser le pipeline existant de backtest + rapports (scripts/run_backtest.py, reports/report_generator.py) sans dupliquer la logique.
 Impl√©menter une fonction utilitaire (nouveau module ou extension de run_backtest.py) du style
generate_best_trial_report(config_path: Path, best_params: dict[str, Any]) -> Path qui :
 Charge la config de backtest de base (soit un YAML d√©di√©, soit la partie ‚Äúbacktest‚Äù dans le YAML d‚Äôoptimisation si pr√©vu),
 Fusionne les best_params Optuna avec les param√®tres de la strat√©gie (merge_params existe d√©j√† dans run_backtest.py),
 Lance le backtest via les fonctions internes (pas forc√©ment via la CLI) pour obtenir metrics/equity/trades,
 Appelle reports.report_generator.generate_report(...),
 Retourne le chemin du HTML g√©n√©r√© (reports/generated/...).
 Dans le dashboard Streamlit :
 Ajouter un bouton ‚ÄúG√©n√©rer rapport backtest (meilleur essai)‚Äù disponible quand le job est done et que best_params sont connus,
 Appeler generate_best_trial_report(...) dans un contexte non bloquant si n√©cessaire,
 Afficher soit :
 un lien vers le fichier (st.markdown("[Voir rapport](file:///...)" ou √©quivalent adapt√©),
 ou un st.components.v1.html(open(path).read(), height=...) pour int√©gration directe.
7. Rapports d‚Äôoverfitting

 S‚Äôappuyer sur scripts/run_overfitting.py et optimization/overfitting_check.py, qui savent d√©j√† :
 Charger la strat√©gie + param_space,
 Utiliser --use-best-params pour r√©cup√©rer les param√®tres optimaux √† partir de best_params_path,
 G√©n√©rer des rapports HTML (WFA, OOS, Monte Carlo, Stability) dans results/overfitting/....
 Ajouter dans visualization/dashboard.py :
 Une section ‚ÄúOverfitting‚Äù affich√©e une fois l‚Äôoptimisation termin√©e,
 Un bouton ‚ÄúLancer checks d‚Äôoverfitting‚Äù qui :
 V√©rifie la pr√©sence du fichier best_params_path (config output.best_params_path dans le YAML d‚Äôoptimisation),
 D√©marre un process s√©par√© pour
python scripts/run_overfitting.py --config <m√™me YAML> --use-best-params,
 Sauvegarde le r√©pertoire racine des sorties (checker.output_root) ou l‚Äôinf√®re √† partir du log / convention (timestamp).
 Impl√©menter une fonction locate_overfitting_index(config_path: Path) -> Path | None qui :
 Inspecte results/overfitting/<run_id>/ pour trouver le dernier run (par timestamp),
 Retourne index.html si pr√©sent.
 Dans Streamlit :
 Afficher un lien ou int√©gration HTML pour index.html (page globale overfitting),
8. Gestion des erreurs & robustesse

 Ajouter du logging coh√©rent (via utils.logger.setup_logger) pour :
 Les d√©marrages de jobs (optimisation, overfitting),
 Les erreurs de chargement de YAML,
 Les probl√®mes de connexion √† la base SQLite Optuna,
 Les erreurs dans la g√©n√©ration de rapports HTML.
 Dans le dashboard, afficher des messages utilisateurs clairs en cas d‚Äôerreur (sans stacktrace brute).
 Pr√©voir un m√©canisme simple pour ‚Äúr√©initialiser‚Äù l‚Äô√©tat :
 Bouton ‚ÄúR√©initialiser dashboard‚Äù qui efface l‚Äô√©tat courant (fichiers de lock / job courant) et permet de relancer une optimisation proprement.
9. Respect du manifeste GEMINI

 V√©rifier que tous les nouveaux modules/fonctions :
 Ont des docstrings claires (Google/Numpy style) expliquant le ‚ÄúPourquoi/Comment‚Äù.
 Utilisent des noms explicites (optimization_job_status, generate_best_trial_report, etc.).
 Restent simples (KISS) : √©viter d‚Äôintroduire un scheduler complexe ou une queue externe tant que ce n‚Äôest pas n√©cessaire.
 S√©parent la logique de donn√©es/optimisation (optimization/, scripts/) de la pr√©sentation (visualization/dashboard.py). -->

üèóÔ∏è Phase 10 : Strat√©gies Avanc√©es (Semaine 9-10)

10.2 Strat√©gies complexes

 Mean reversion strategy
 Momentum breakout
 Pairs trading
 R√©gime detection

10.3 Machine Learning prep
IA comme Filtre de R√©gime
 Feature engineering
 Labeling des donn√©es
 Setup pour ML (optionnel)

<!-- üß™ Phase 11 : Testing et Validation (Semaine 10-11)
11.1 Unit tests

 Tests pour data_manager
 Tests pour strategies
 Tests pour risk management
 Tests pour indicators -->

11.2 Integration tests

 Test pipeline complet
 Test avec donn√©es corrompues
 Test edge cases

11.3 Performance tests

 Benchmark vitesse backtest
 Optimisation du code
 Profiling m√©moire

Phase 12 : Architecture de Trading Live (Paper & Live)
    - [ ] 12.1 Couche d'Abstraction Broker
        - [ ] Cr√©er une interface `BaseBroker` (avec m√©thodes `submit_order`, `get_position`, `get_account_balance`).
        - [ ] Cr√©er une impl√©mentation `BacktestBroker` (qui wrappe le broker de Backtrader).
        - [ ] Cr√©er une impl√©mentation `PaperBroker` (Alpaca).
    - [ ] 12.2 Moteur d'√âv√©nements (Event-Driven Engine)
        - [ ] Migrer de la boucle `next()` de Backtrader √† une boucle d'√©v√©nements (Event Loop : `MarketEvent`, `SignalEvent`, `OrderEvent`, `FillEvent`).
        - [ ] *Objectif :* Utiliser la *m√™me* logique de strat√©gie pour le backtest et le live.
    - [ ] 12.3 Service de Monitoring & Alertes
        - [ ] Logger les ex√©cutions d'ordres vers un canal d√©di√© (Telegram).
        - [ ] Mettre en place un "Heartbeat" (service qui v√©rifie que le trader tourne toujours).

üéØ Phase 13 : Scanner & Gestion de Portefeuille
    - [ ] 13.1 Scanner de March√©
        - [ ] `scripts/live_scanner.py` doit √™tre un service ind√©pendant (ex: cron job).
        - [ ] Le scanner ne trade pas ; il *g√©n√®re* des signaux (ex: "AAPL - Tendance Haussi√®re H1") et les stocke (ex: dans un fichier, une DB Redis, ou une DB SQL).
    - [ ] 13.2 Gestionnaire de Portefeuille (Le "Cerveau")
        - [ ] Cr√©er une classe `PortfolioManager` qui s'ex√©cute apr√®s le Scanner.
        - [ ] *Logique :* Lire les signaux du Scanner, v√©rifier les positions actuelles, et allouer le capital (en utilisant `risk_management/position_sizing.py`).
        - [ ] G√©rer les conflits (ex: 5 signaux d'achat mais capital pour 2 trades).
        - [ ] G√©rer l'allocation inter-strat√©gies (Que faire si 2 strat√©gies diff√©rentes veulent acheter le m√™me actif ?).

Phase 14 : Pipeline de Donn√©es de Production
    - [ ] 14.1 Fournisseur de Donn√©es
        - [ ] S√©lectionner un fournisseur de donn√©es H1/Daily (payant ou API fiable, ex: Alpaca, IEX, EODHistoricalData).
    - [ ] 14.2 Base de Donn√©es Temporelle (TSDB)
        - [ ] Mettre en place une base de donn√©es optimis√©e pour les s√©ries temporelles (ex: InfluxDB, TimescaleDB, ou m√™me un stockage Parquet/S3).
        - [ ] Cr√©er un "ETL" qui peuple cette base de donn√©es (en dehors du script de trading).
    - [ ] 14.3 Mise √† jour du DataManager
        - [ ] `utils/data_manager.py` doit √™tre modifi√© pour lire depuis cette nouvelle base de donn√©es (en live) au lieu des fichiers CSV (en backtest).

üìà M√©triques de Succ√®s du Projet
Court terme (1 mois)

 Backtest fonctionnel sur 3+ strat√©gies
 Sharpe ratio > 1 sur donn√©es historiques
 Syst√®me d'optimisation automatique

Moyen terme (3 mois)

 10+ strat√©gies test√©es
 Walk-forward validation positive
 Paper trading actif

Long terme (6 mois)

 Syst√®me de production stable
 ROI positif en paper trading
 Documentation compl√®te

üîÑ Maintenance Continue
Hebdomadaire

 Revue des performances
 Mise √† jour des donn√©es
 Check des logs d'erreurs

Mensuelle

 R√©-optimisation des param√®tres
 Analyse des drawdowns
 Mise √† jour documentation

Trimestrielle

 Revue strat√©gie globale
 Benchmarking vs march√©
 Planification nouvelles features
